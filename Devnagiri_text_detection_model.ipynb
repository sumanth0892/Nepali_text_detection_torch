{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9e277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import torch\n",
    "import codecs\n",
    "import random\n",
    "import numpy as np\n",
    "import editdistance\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad6efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_char_list = 'charList.txt'\n",
    "hindi_vocab = 'hindi_vocab.txt'\n",
    "with codecs.open('full.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "lines = [x.strip() for x in lines]\n",
    "chars = set()\n",
    "print(*lines[:5])\n",
    "batchSize = 32\n",
    "img_size = (128, 32)\n",
    "max_text_len = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e8234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, imgSize, dataAugmentation=False):\n",
    "\t\"put img into target img of size imgSize, transpose for TF and normalize gray-values\"\n",
    "\n",
    "\t# there are damaged files in IAM dataset - just use black image instead\n",
    "\tif img is None:\n",
    "\t\timg = np.zeros([imgSize[1], imgSize[0]])\n",
    "\n",
    "\t# create target image and copy sample image into it\n",
    "\t(wt, ht) = imgSize\n",
    "\t(h, w) = img.shape\n",
    "\tfx = w / wt\n",
    "\tfy = h / ht\n",
    "\tf = max(fx, fy)\n",
    "\tnewSize = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1)) # scale according to f (result at least 1 and at most wt or ht)\n",
    "\timg = cv2.resize(img, newSize)\n",
    "\ttarget = np.ones([ht, wt]) * 255\n",
    "\ttarget[0:newSize[1], 0:newSize[0]] = img\n",
    "\n",
    "\t# transpose for TF\n",
    "\timg = cv2.transpose(target)\n",
    "\n",
    "\t# normalize\n",
    "\t(m, s) = cv2.meanStdDev(img)\n",
    "\tm = m[0][0]\n",
    "\ts = s[0][0]\n",
    "\timg = img - m\n",
    "\timg = img / s if s>0 else img\n",
    "\treturn img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa2421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    def __init__(self, gt_text, file_path):\n",
    "        self.gt_text = gt_text\n",
    "        self.file_path = file_path\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    def __init__(self, gt_texts, imgs):\n",
    "        self.imgs = np.stack(imgs, axis = 0)\n",
    "        self.gt_texts = gt_texts\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, batch_size, img_size, max_text_len):\n",
    "        \"\"\"\n",
    "        Loader for the dataset\n",
    "        :param file_path: File path of the image\n",
    "        :param batch_size: Batch size\n",
    "        :param img_size: Size of the image\n",
    "        :param max_text_len: Maximum text length\n",
    "        \"\"\"\n",
    "        self.data_augmentation = False\n",
    "        self.cur_idx = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.samples = []\n",
    "\n",
    "        with codecs.open(\"full.txt\", 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        lines = [x.strip() for x in lines]\n",
    "        chars = set()\n",
    "        print(lines[5])\n",
    "        for line in lines:\n",
    "            if not line or line[0] == '#':\n",
    "                continue\n",
    "            line_split = line.strip().split(' ')\n",
    "            if line_split[0] == '\\ufeff':\n",
    "                continue\n",
    "            file_name = line_split[0]\n",
    "\n",
    "            # Ground Truth text starts at column 1\n",
    "            gt_text = self.truncate_label(' '.join(line_split[1]), max_text_len)\n",
    "            chars = chars.union(set(list(gt_text)))\n",
    "\n",
    "            # Check if image not empty\n",
    "            if not os.path.getsize(file_name):\n",
    "                continue\n",
    "            self.samples.append(Sample(gt_text, file_name)) # This can be a dictionary\n",
    "\n",
    "        # Split into training, validation and testing sets\n",
    "        n1, n2 = int(0.8*len(self.samples)), int(0.9*len(self.samples))\n",
    "        self.train_samples = self.samples[:n1]\n",
    "        self.validation_samples = self.samples[n1:n2]\n",
    "        self.test_samples = self.samples[n2:]\n",
    "\n",
    "        # Put words into lists\n",
    "        self.train_words = [x.gt_text for x in self.train_samples]\n",
    "        self.test_words = [x.gt_text for x in self.test_samples]\n",
    "        self.valid_words = [x.gt_text for x in self.validation_samples]\n",
    "\n",
    "        # Number of randomly chosen samples per epoch\n",
    "        self.num_train_samples_per_epoch = 10000\n",
    "\n",
    "        self.train_set()\n",
    "\n",
    "        # List of chars in the dataset\n",
    "        self.char_list = sorted(list(chars))\n",
    "\n",
    "    @staticmethod\n",
    "    def truncate_label(text, max_text_len):\n",
    "        cost = 0\n",
    "        for i in range(len(text)):\n",
    "            if i != 0 and text[i] == text[i - 1]:\n",
    "                cost += 2\n",
    "            else:\n",
    "                cost += 1\n",
    "            if cost > max_text_len:\n",
    "                return text[:i]\n",
    "        return text\n",
    "\n",
    "    def train_set(self):\n",
    "        \"\"\"\n",
    "        Switch to randomly chosen subset of training set\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.data_augmentation = True\n",
    "        self.cur_idx = 0\n",
    "        random.shuffle(self.train_samples)\n",
    "        self.samples = self.train_samples[:self.num_train_samples_per_epoch]\n",
    "\n",
    "    def validation_set(self):\n",
    "        \"\"\"\n",
    "        Switch to validation set\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.data_augmentation = False\n",
    "        self.cur_idx = 0\n",
    "        random.shuffle(self.validation_samples)\n",
    "        self.samples = self.validation_samples\n",
    "\n",
    "    def test_set(self):\n",
    "        \"\"\"\n",
    "        Switch to testing set\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.data_augmentation = False\n",
    "        self.cur_idx = 0\n",
    "        random.shuffle(self.test_samples)\n",
    "        self.samples = self.test_samples\n",
    "\n",
    "    def get_iterator_info(self):\n",
    "        \"\"\"\n",
    "        Current batch index and total number of batches\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.cur_idx // self.batch_size + 1, len(self.samples) // self.batch_size\n",
    "\n",
    "    def has_next(self):\n",
    "        return self.cur_idx + self.batch_size <= len(self.samples)\n",
    "\n",
    "    def get_next(self):\n",
    "        batch_range = range(self.cur_idx, self.cur_idx + self.batch_size)\n",
    "        gt_texts = [self.samples[i].gt_text for i in batch_range]\n",
    "        imgs = [preprocess(cv2.imread(self.samples[i].file_path, cv2.IMREAD_GRAYSCALE), self.img_size, self.data_augmentation) for i in batch_range]\n",
    "        self.cur_idx += self.batch_size\n",
    "        return Batch(gt_texts, imgs)\n",
    "dataloader = DataLoader(batchSize, img_size, max_text_len)\n",
    "print(len(dataloader.char_list))\n",
    "open(fn_char_list, 'w', encoding = 'utf-8').write(str().join(dataloader.char_list))\n",
    "open(hindi_vocab, 'w', encoding='UTF-8').write(str(' ').join(dataloader.train_words + dataloader.valid_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c0c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "class ViTHTRModel(nn.Module):\n",
    "    def __init__(self, char_list, img_size=(128, 32), patch_size=8, embed_dim=128,max_text_len = 32 ,num_heads=8, num_layers=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.char_list = char_list\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size[0] // patch_size) * (img_size[1] // patch_size)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_text_len = max_text_len\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        # Position embedding\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embed_dim, num_heads, embed_dim * 4, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(embed_dim, len(char_list))  # +1 for CTC blank\n",
    "        \n",
    "        # Positional encoding for transformer input\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        self.criterion = nn.CTCLoss()\n",
    "        self.optimizer = torch.optim.RMSprop(self.parameters(), lr=0.001)\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) != 4:\n",
    "            x = x.unsqueeze(1)\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)\n",
    "        print(f\"After applying the patch embeddings, the shape is {x.shape}\")\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        print(f\"After flattening and transposing, the shape is {x.shape}\")\n",
    "        \n",
    "        # Add position embedding\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        print(f\"After applying the position embeddings the shape is {x.shape}\")\n",
    "        \n",
    "        # Positional encoding\n",
    "        x = x.transpose(0, 1)\n",
    "        print(f\"Prior to positional encoding the shape is {x.shape}\")\n",
    "        x = self.pos_encoder(x)\n",
    "        print(f\"After applying the positional encoding the shape is {x.shape}\")\n",
    "        \n",
    "        # Transformer layers\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "        print(f\"After the transformer layer, the shape is {x.shape}\")\n",
    "        \n",
    "        # Output layer\n",
    "        #x = x.transpose(0, 1) # Has to be changed depending on the model's output and errors.\n",
    "        x = self.fc(x)\n",
    "        print(f\"After the final connected layer the shape is {x.shape}\")\n",
    "        \n",
    "        # Apply log softmax for CTC loss\n",
    "        log_probs = F.log_softmax(x, dim=-1)\n",
    "        print(f\"After the log softmax of the final connected layer the shape is {log_probs.shape}\")\n",
    "        print(f\"The min value in the log softmax probs is{torch.min(log_probs)}\")\n",
    "        print(f\"The max value in the log softmax probs is{torch.max(log_probs)}\")\n",
    "        \n",
    "        return log_probs\n",
    "    \n",
    "    def to_sparse(self, texts):\n",
    "        \"\"\"put ground truth texts into sparse tensor for ctc_loss\"\"\"\n",
    "        indices, values = [], []\n",
    "        shape = [len(texts), 0]  # last entry must be max(labelList[i])\n",
    "\n",
    "        for (batchElement, text) in enumerate(texts):\n",
    "            label_str = [self.char_list.index(c) for c in text]\n",
    "            if len(label_str) > shape[1]:\n",
    "                shape[1] = len(label_str)\n",
    "            for (i, label) in enumerate(label_str):\n",
    "                indices.append([batchElement, i])\n",
    "                values.append(label)\n",
    "\n",
    "        return indices, values, shape\n",
    "\n",
    "    def decoder_output_to_text(self, ctc_output, batch_size):\n",
    "        \"\"\"extract texts from output of CTC decoder\"\"\"\n",
    "        print(f\"Maximum value of the CTC output tensor is {torch.max(ctc_output)} and minimum value {torch.min(ctc_output)}\")\n",
    "        encoded_label_strs = [[] for _ in range(batch_size)]\n",
    "        # Convert to a column vector\n",
    "        ctc_output = ctc_output.unsqueeze(1)\n",
    "        print(f\"The output logits are of shape {ctc_output.shape}\")\n",
    "        blank = len(self.char_list)\n",
    "        for b in range(batch_size):\n",
    "            for label in ctc_output[b]:\n",
    "                if label == blank:\n",
    "                    break\n",
    "                encoded_label_strs[b].append(label)\n",
    "        return [\"\".join([self.char_list[c] for c in labelStr]) for labelStr in encoded_label_strs]\n",
    "    \n",
    "    def train_batch(self, batch):\n",
    "        \"\"\"feed a batch into the NN to train it\"\"\"\n",
    "        self.train()\n",
    "        num_batch_elements = len(batch.imgs)\n",
    "        # indices = sparse[0], values = sparse[1], shape = sparse[2]\n",
    "        sparse = self.to_sparse(batch.gt_texts) \n",
    "        imgs = torch.tensor(batch.imgs, dtype=torch.float32)\n",
    "        if imgs.dim() == 3:\n",
    "            imgs = imgs.unsqueeze(1)\n",
    "        \n",
    "        gt_texts = torch.sparse_coo_tensor(\n",
    "        torch.LongTensor(sparse[0]).t(),\n",
    "        torch.LongTensor(sparse[1]),\n",
    "        torch.Size(sparse[2])\n",
    "        ).to_dense()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        log_probs = self.forward(imgs)\n",
    "        \n",
    "        # prepare input lengths (need to understand more about this!!)\n",
    "        input_lengths = torch.full((num_batch_elements,), log_probs.size(0), dtype=torch.long)\n",
    "        #print(f\"Log probs first dimension is {log_probs.size(0)}\")\n",
    "        #print(f\"batch size is {num_batch_elements}\")\n",
    "        #print(f\"Normalized logits are of shape {log_probs.shape}\")\n",
    "        #print(f\"Ground truth texts are of shape {gt_texts.shape}\")\n",
    "        #print(f\"Input lengths is of shape {input_lengths.shape}\")\n",
    "        \n",
    "        # prepare target lengths\n",
    "        target_lengths = torch.LongTensor([len(t) for t in batch.gt_texts])\n",
    "        #print(f\"Target lengths is of shape {target_lengths.shape}\")\n",
    "        loss = self.criterion(log_probs, gt_texts, input_lengths, target_lengths)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def infer_batch(self, batch, calc_probability=False, probability_of_gt=False):\n",
    "        \"\"\"feed a batch into the NN to recognize the texts\"\"\"\n",
    "        self.eval()\n",
    "        num_batch_elements = len(batch.imgs)\n",
    "        imgs = torch.tensor(batch.imgs, dtype=torch.float32)\n",
    "        seq_len = torch.tensor([self.max_text_len] * num_batch_elements, dtype=torch.int32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = self.forward(imgs)\n",
    "            print(f\"The predictions in the infer_batch() are of shape {preds.shape}\")\n",
    "            preds = preds.transpose(0,1)\n",
    "            _, preds = preds.max(-1)\n",
    "            print(f\"The predictions in the infer_batch() are of shape {preds.shape}\")\n",
    "            preds = preds.transpose(1,0).contiguous().view(-1)\n",
    "\n",
    "        texts = self.decoder_output_to_text(preds, num_batch_elements)\n",
    "\n",
    "        probs = None\n",
    "        if calc_probability:\n",
    "            sparse = self.to_sparse(batch.gt_texts) if probability_of_gt else self.to_sparse(texts)\n",
    "            gt_texts = (torch.LongTensor(sparse[0]), torch.LongTensor(sparse[1]), torch.Size(sparse[2]))\n",
    "            loss = self.criterion(preds, gt_texts, seq_len, torch.IntTensor([self.max_text_len] * num_batch_elements))\n",
    "            probs = torch.exp(-loss).cpu().numpy()\n",
    "\n",
    "        return texts, probs\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"save model to file\"\"\"\n",
    "        self.snap_id += 1\n",
    "        torch.save(self.state_dict(), f'../model/snapshot_{self.snap_id}.pth')\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"load model from file\"\"\"\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "# Usage\n",
    "char_list = dataloader.char_list  # Example character list\n",
    "model = ViTHTRModel(char_list)\n",
    "\n",
    "# Test the model\n",
    "dummy_input = torch.randn(32, 1, 128, 32)\n",
    "output = model(dummy_input)\n",
    "print(\"Model output shape:\", output.shape)\n",
    "summary(model, (1, 128, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3c4d48",
   "metadata": {},
   "source": [
    "#### Training and validating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1c0a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    loader.validation_set()\n",
    "    num_char_err = 0\n",
    "    num_char_total = 0\n",
    "    num_word_0K = 0\n",
    "    num_word_total = 0\n",
    "\n",
    "    while loader.has_next():\n",
    "        iter_info = loader.get_iterator_info()\n",
    "        print(f\"Batch: {iter_info[0], '/', iter_info[1]}\")\n",
    "        batch = loader.get_next()\n",
    "        (recognized, _) = model.infer_batch(batch)\n",
    "\n",
    "        print(f\"Ground Truth -> Recognized\")\n",
    "        for i in range(len(recognized)):\n",
    "            num_word_0K += 1 if batch.gt_texts[i] == recognized[i] else 0\n",
    "            num_word_total += 1\n",
    "            dist = editdistance.eval(recognized[i], batch.gt_texts[i])\n",
    "            num_char_err += dist\n",
    "            num_char_total += len(batch.gt_texts[i])\n",
    "            print('[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' + batch.gt_texts[i] + '\"', '->',\n",
    "                  '\"' + recognized[i] + '\"')\n",
    "    return num_char_err\n",
    "\n",
    "def train(model, loader):\n",
    "    val_error_rates, losses = [], []\n",
    "    best_error_rate = float(\"inf\")\n",
    "    no_improvement_since = 0\n",
    "    early_stopping = 5\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        print(f\"Currently on epoch {epoch}\")\n",
    "        \n",
    "        # Training\n",
    "        print(\"Training\")\n",
    "        loader.train_set()\n",
    "        while loader.has_next():\n",
    "            iter_info = loader.get_iterator_info()\n",
    "            batch = loader.get_next()\n",
    "            loss = model.train_batch(batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Validation\n",
    "        char_error_rate = validate(model, loader)\n",
    "        \n",
    "        # If this is the least error rate, then save the model parameters\n",
    "        if char_error_rate < best_error_rate:\n",
    "            print(\"Character error rate improved\")\n",
    "            no_improvement_since = 0\n",
    "            val_error_rates.append(char_error_rate)\n",
    "        else:\n",
    "            print(\"Error rate not improved\")\n",
    "            no_improvement_since += 1\n",
    "        \n",
    "        # Stop if there's no improvement in error\n",
    "        if no_improvement_since > early_stopping:\n",
    "            print(\"Training stopped\")\n",
    "            break\n",
    "    print(losses)\n",
    "\n",
    "train(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076781dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.train_set()\n",
    "batch = dataloader.get_next()\n",
    "print(batch.gt_texts)\n",
    "texts = batch.gt_texts\n",
    "print(*texts)\n",
    "print(\"\\n\")\n",
    "print(*dataloader.char_list)\n",
    "images = batch.imgs\n",
    "texts = batch.gt_texts\n",
    "log_probs = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2502ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batch_elements = len(images)\n",
    "print(f\"There are {num_batch_elements} images in the batch\")\n",
    "texts = batch.gt_texts\n",
    "print(f\"The Ground truth texts for this batch are of shape {gt_texts.shape}\")\n",
    "print(f\"The images are of shape {images.shape}\")\n",
    "model.optimizer.zero_grad()\n",
    "print(f\"The logarithmic probabilities are of shape {log_probs.shape}\")\n",
    "print(f\"The input lengths are of shape {input_lengths.shape}\")\n",
    "print(f\"The target lengths are of shape {target_lengths.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 kernel",
   "language": "python",
   "name": "python310_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
